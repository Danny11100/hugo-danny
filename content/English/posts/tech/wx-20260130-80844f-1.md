---
title: "Information Noise Through the Lens of the Attention Model"
date: 2026-01-30
lastmod: 2026-01-30
draft: false
author: "Danny Yuan"
categories:
  - tech
tags: []
slug: wx-20260130-80844f
comments: true
showToc: true
ShowReadingTime: true
ShowWordCounts: true
ShowPageViews: true
ShowLastMod: true
cover:
  image: "/wechat/wx-20260130-80844f/cover.jpg"
  caption: ""
  alt: ""
  relative: false
description: "Attention models fight overload; attention economics amplifies stimulation. The only move is to take back allocation control."
---

I was watching Andrew Ng’s LLM course. When he explained the attention mechanism in Transformers, I connected the technical concept with my recent state.

![](/wechat/wx-20260130-80844f/img-01.jpg)
![](/wechat/wx-20260130-80844f/img-02.jpg)

The attention mechanism solves a simple problem: too much information, limited compute. A model can’t treat all inputs equally; it must decide what matters now and what can be ignored. If attention is misallocated, it loses the main thread and outputs become messy.

This maps directly to real life. Our information environment is also an attention allocation system. Short videos, feeds, hot events—all compete for the same scarce resource: human attention. The difference is in rules. Model attention is driven by semantic relevance and task goals; the real‑world attention economy is driven by emotional feedback and dwell time. The more emotional, the more it gets pushed.

What I felt most was noise. Scrolling short videos became hard to stop. The content itself wasn’t important, but the pacing was dense and stimulus continuous. Afterward, nothing accumulated—just irritation. AI news on X is similar: new models, new judgments, new opportunities flood your screen. The same thing gets repackaged over and over. It looks like high information, but very little actually affects decisions. It’s all AIAIAI—either “shocking” or “world‑changing”—while my dinner is still McDonald’s, at most switching from teriyaki chicken to Angus beef.

More noise drags emotions. The subtler damage is decision‑making: more hesitation, more anxiety—not because there’s too little info, but because there’s too much and it conflicts. In model terms, it’s a classic failure: all inputs get high weights, so the system can’t focus.

I started actively adjusting my inputs. I turned off the video‑feed入口 entirely. On X, I now use mute and block more than refresh. Instead of filtering in noise, I use negative feedback to down‑weight obvious distractors.

I gradually realized the problem isn’t whether a single piece is worth reading; it’s the source. Many accounts just搬运 and amplify emotion—fast,刺激, but low long‑term value. True primary sources update slowly, avoid premature conclusions, and feel more stable.

So I began focusing on sources rather than content. Is there long‑term incremental value? Clear boundaries? Restraint toward uncertainty? If an account keeps manufacturing anxiety, I mute it even if it looks “hot.”

From the attention‑model perspective, this is engineering optimization: use negative feedback to reduce the weight of low‑value inputs, so limited attention can focus on more reliable signals.

Attention models fight information overload; attention economics amplifies stimulation. We sit between them. The move isn’t complex: slowly take back the power to allocate attention. When inputs are cleaner, emotions stabilize, and decisions become clearer.
